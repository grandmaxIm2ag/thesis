\chapter{Introduction}
\minitoc

\section{Context}

\section{Problem Statement}

Hand-coding Planning domains is generally considered difficult, tedious and error-prone. The reason is that experts in charge modelling domains are not always PDDL experts and vice versa. To overcome this issue, two main approaches have been proposed. One is to develop knowledge engineering tools facilitating PDDL writing. These tools provide support for consistency and syntactic error checking, domain visualisation etc. An inconvenient aspect of these tools is that they require PDDL expertise and background in software engineering \cite{shah2013knowledge}.

The other approach is to develop machine learning algorithms to automatically generate Pla domains as, for instance, ARMS \cite{arms}, SLAF \cite{slaf}, LSONIO \cite{lsonio}, LOCM \cite{locm}. These algorithms use training datasets made of possibly noisy and partial state/action sequences generated by a planner, or randomly generated. Classically, IPC benchmarks are used to generate training datasets. The performance of these algorithms is  then measured as the syntactical distance between the learned domains and the IPC benchmarks. Machine learning approaches have three main drawbacks.

First, most of these approaches require a lot of data to perform the learning of Planning domains and in many real world applications, acquiring training datasets is difficult and costly, e.g., Mars Exploration Rover operations \cite{bresina}. Also, dataset acquisition is a long term evolutive process: in real world applications, training data become available gradually over time. Moreover, in practice, it is important to be able to update learned PDDL domains to new incoming data without restarting the learning process from scratch.

Second, the learned domains are not {\em accurate} enough to be used "as is" in a planner: a step of expert proofreading is still necessary to correct them. Even small syntactical errors can make sometime the learned domains useless for planning. Therefore, we consider that domain {\em accuracy}, that we define as the capacity of a learned domain to solve planning problems that were not used in the training dataset, is a better performance indicator than syntactical distance in practice.

Third, even if some approaches, e.g., \cite{lsonio,planMilner,irale} are able to learn from noisy and/or partially observable data, few approaches are able to handle very high levels of noise and partial observations as can be encountered in real world applications, especially in robotics where observations are extracted using miscalibrated or noisy sensors.

\section{The AMLSI Approach}

To address these challenges, we propose a novel Planning domain learning algorithm called AMLSI ({\em Action Model Learning with State machine Interaction}). A key idea in AMLSI is that real world can be modelled as state machines and that retro-engineering real world
state machines is analogous to learning a Deterministic Finite Automaton (DFA), which is equivalent to a regular grammar: we argue that (1) it is possible to learn a DFA by querying a real world state machine (see Figure~\ref{fig:amlsi}), and (2) to induce a PDDL domain from this regular grammar. AMLSI does not require any prior knowledge regarding the feasibility of actions in a given state, and state observations can be partial and noisy. AMLSI is highly accurate even with highly partial and noisy state observations. Thus it minimizes PDDL proofreading and correction for domain experts. Our experiments show that in many cases AMLSI does not even require any correction of the learned domains. AMLSI is lean and efficient on data consumption. It uses a supervised learning approach based on grammar induction. Training data are action/state (possibly partial and noisy) sequences labeled as feasible or infeasible. Both, feasible and infeasible action/state sequences are used by AMLSI to learn PDDL domains, thus maximizing data usability.

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.2\linewidth}
        \centering
        \includegraphics[width=.9\linewidth]{pics/out/amlsi_steps.pdf}
        \caption{AMLSI: {\sf PDDL} domain learning overview}
        \label{fig:contributions:pddl}
    \end{subfigure}
    \begin{subfigure}[b]{.4\linewidth}
        \centering
        \includegraphics[width=.9\linewidth]{pics/out/incremental_amlsi.pdf}
        \caption{IncrAMLSI: Incremental {\sf PDDL} domain learning overview}
        \label{fig:contributions:incremental}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\linewidth}
        \centering
        \includegraphics[width=.9\linewidth]{pics/out/temporal_amlsi.pdf}
        \caption{TempAMLSI: {\sf Temporal} domain learning overview}
        \label{fig:contributions:temporal}
    \end{subfigure}
    \caption{Contributions overview}
    \label{fig:contributions}
\end{figure*}

\subsection{Observation Generation}

AMLSI produces a set of observations by using a random walk. AMLSI is able to efficiently exploit these observations by taking into account not only the fact that some actions are applicable in certain states but also that others are not. More precisely, to generate the observations, AMLSI uses random walks by applying a randomly selected action to the initial state of the problem. If this action is feasible, it is appended to the current action sequence. This procedure is repeated until the selected action is not feasible in the current state. The feasible prefix of the action sequence is then added to the set of positive samples, and the complete sequence, whose last action is not feasible, is added to the set of negative samples. This generation method allow to maximize data usability in situations where obtaining training datasets is costly and/time-consuming.

\subsection{Domain Learning}

\paragraph{PDDL Learning}
An overview of the AMLSI \cite{amlsi_ictai, amlsi_keps} algorithm is shown in Figure~\ref{fig:contributions:pddl}. AMLSI is composed of three steps:
\begin{enumerate}
    \item {\em DFA Learning:} AMLSI uses an alternative version of the AMLSI algorithm with the DFA learning algorithm proposed by \cite{amlsi_ictai}.
    \item {\em Operator Generation:} AMLSI generates the set of PDDL operators from the learned grammar DFA with the observed states.
    \item {\em Operator Refinement:} AMLSI refines the operator preconditions and effects. This refinement steps is necessary to address partial and noisy observations.
\end{enumerate}

\paragraph{Incremental Learning}
An overview of the IncrAMLSI algorithm \cite{incr} is shown in Figure~\ref{fig:contributions:incremental}. IncrAMLSI learns incrementally the PDDL domain from incoming data and does not restart from scratch when new data become available at each iteration $t$. More precisely, IncrAMLSI consists in incrementally updating the PDDL domain with the new incoming training datasets available at iteration $t$ to produce the new domain. This algorithm is made up of three steps:
    \begin{enumerate}
        \item {\em Update of the DFA (regular grammar)} with the DFA learning algorithm proposed by \cite{amlsi_ictai}.
        \item {\em Overhaul of the PDDL operators} in order to add new operators and remove preconditions and effects that are no longer compatible with positive and negative samples, and the updated DFA at iteration $t$.
        \item {\em Refinement of the PDDL operators} as in AMLSI algorithm to deal with noisy and partial states in observations.
    \end{enumerate}
The incremental process is operated each time new training datasets are input and until convergence of the PDDL domain.

\paragraph{Temporal Learning}
Some planners, such as Crikey \cite{crikey}, solve {\sf Temporal }Problems by using non-temporal planners. To that end, they convert {\sf Temporal }Problems into {\sf PDDL} planning problems, solve them with a non-temporal planner. Then they convert the classical plan into a temporal plan with rescheduling techniques. TempAMLSI \cite{temp} reuse this idea in order to learn {\sf Temporal} domains: TempAMLSI learns a {\sf PDDL} domain and then infer a {\sf Temporal } domain. TempAMLSI (summarized in Figure \-- \ref{fig:contributions:temporal}) has three steps.
\begin{enumerate}
    \item {\em Sample Translation:} After having generated the samples of {\it temporal} sequences (including both feasible and infeasible sequences), TempAMLSI translates them into non-temporal sequences.
    \item {\em PDDL Domain Learning:} TempAMLSI uses AMLSI to learn an intermediate and classical {\sf PDDL} domain.
    \item {\em Domain Translation:} TempAMLSI translates the PDDL domain into a {\sf Temporal} domain.
\end{enumerate}
This algorithm is able to learn both Sequential and SHE accurate {\sf Temporal} domains.


\section{Document Organization}
